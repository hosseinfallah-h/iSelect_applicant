# -*- coding: utf-8 -*-
"""
I-SELECT (Applicant Intake) â€” Enhanced with improved capabilities extraction
"""

import os
import csv
import re
import json
import tempfile
from datetime import datetime
from typing import Dict, List, Optional

import pandas as pd
from flask import Flask, render_template, request, jsonify

# ---- Enhanced Dependencies ----
try:
    import ollama
except ImportError:
    ollama = None
    print("âš ï¸ Ollama not available. Install: pip install ollama")

try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0
except ImportError:
    detect = None
    print("âš ï¸ langdetect not available. Install: pip install langdetect")

try:
    import PyPDF2
except ImportError:
    PyPDF2 = None
    print("âš ï¸ PyPDF2 not available. Install: pip install PyPDF2")

try:
    from docx import Document
except ImportError:
    Document = None
    print("âš ï¸ python-docx not available. Install: pip install python-docx")

# -----------------------------------------------------------------------------
# App config
# -----------------------------------------------------------------------------
app = Flask(__name__)
app.config["DATA_FOLDER"] = "data"
app.config["EXCEL_PATH"] = os.path.join(app.config["DATA_FOLDER"], "people.xlsx")
app.config["UPLOAD_FOLDER"] = os.path.join(app.config["DATA_FOLDER"], "uploads")
app.config["NAME_LEXICON_PATH"] = os.path.join(app.config["DATA_FOLDER"], "names_fa.csv")
os.makedirs(app.config["DATA_FOLDER"], exist_ok=True)
os.makedirs(app.config["UPLOAD_FOLDER"], exist_ok=True)

OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3:1b")

# -----------------------------------------------------------------------------
# Enhanced Normalizers with Language Detection
# -----------------------------------------------------------------------------
PERSIAN_DIGITS = str.maketrans("Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹", "0123456789")
ARABIC_DIGITS  = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")

def norm(s: str) -> str:
    if s is None:
        return ""
    return str(s).strip()

def normalize_digits(s: str) -> str:
    s = str(s or "")
    return s.translate(PERSIAN_DIGITS).translate(ARABIC_DIGITS)

def normalize_spaces(s: str) -> str:
    s = (s or "").replace("\u200c", " ")
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def to_int_or_empty(v):
    if v in (None, "", "null"):
        return ""
    try:
        return int(float(str(v)))
    except Exception:
        return ""

def detect_and_translate(text: str) -> str:
    """Detect language and translate to Persian if needed"""
    if not text or not detect:
        return text
    
    try:
        lang = detect(text)
        if lang != 'fa':
            # Use Ollama for translation
            if ollama:
                response = ollama.chat(
                    model=OLLAMA_MODEL,
                    messages=[{
                        "role": "user", 
                        "content": f"Translate this to Persian: {text}"
                    }]
                )
                return response['message']['content']
    except Exception:
        pass
    
    return text

# -----------------------------------------------------------------------------
# Name Lexicon (Enhanced)
# -----------------------------------------------------------------------------
BUILTIN_MALE = {
    "Ø¹Ù„ÛŒ","Ø­Ø³ÛŒÙ†","Ù…Ø­Ù…Ø¯","Ø±Ø¶Ø§","Ù…Ù‡Ø¯ÛŒ","Ø§Ù…ÛŒØ±","Ø­Ù…ÛŒØ¯","Ø³Ø¹ÛŒØ¯","Ù‡Ø§Ø¯ÛŒ","Ø­Ø§Ù…Ø¯","ÙˆØ­ÛŒØ¯","Ù…ØµØ·ÙÛŒ","Ø­Ø³Ù†","Ù…Ø¬ØªØ¨ÛŒ",
    "Ù…Ø¬ÛŒØ¯","Ù…ÛŒÙ„Ø§Ø¯","Ø§Ø­Ù…Ø¯","Ú©Ø§Ø¸Ù…","Ø¨Ù‡Ø²Ø§Ø¯","Ø±ÙˆØ­â€ŒØ§Ù„Ù„Ù‡","Ø±ÙˆØ­ Ø§Ù„Ù„Ù‡","ÛŒØ§Ø³Ø±","Ù…Ø­Ø³Ù†","Ù†ÛŒÙ…Ø§","Ú©ÛŒØ§Ù†","Ù¾Ø§Ø±Ø³Ø§",
}
BUILTIN_FEMALE = {
    "Ø²Ù‡Ø±Ø§","ÙØ§Ø·Ù…Ù‡","Ù…Ø±ÛŒÙ…","Ø³Ø§Ø±Ø§","Ø³Ù…ÛŒØ±Ø§","Ù…ÛŒÙ†Ø§","Ù…Ù‡Ø³Ø§","Ù†Ø§Ø²Ù†ÛŒÙ†","Ø§Ù„Ù‡Ø§Ù…","Ù¾Ø±ÛŒØ³Ø§","Ù†ÛŒÙ„ÙˆÙØ±","Ø±ÛŒØ­Ø§Ù†Ù‡","Ù†Ú¯Ø§Ø±","Ù‡Ø¯ÛŒÙ‡",
    "Ø±Ø§Ø¶ÛŒÙ‡","Ù…Ø¹ØµÙˆÙ…Ù‡","Ø´Ø¨Ù†Ù…","Ø«Ù†Ø§","Ù…Ù„ÛŒÚ©Ø§","Ø­Ø¯ÛŒØ«","Ø­Ø¯ÛŒØ«Ù‡","ÙØ±Ø´ØªÙ‡","Ø³ÙˆÚ¯Ù†Ø¯","Ø³ØªØ§ÛŒØ´","Ù†Ø±Ú¯Ø³","Ø¢ØªÙ†Ø§","Ø¢ÛŒÙ†Ø§Ø²",
}

def load_name_lexicon():
    male = set(BUILTIN_MALE)
    female = set(BUILTIN_FEMALE)
    path = app.config["NAME_LEXICON_PATH"]
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                reader = csv.reader(f)
                for row in reader:
                    if not row:
                        continue
                    first = (row[0] or "").strip()
                    g_raw = (row[1] if len(row) > 1 else "").strip()
                    if not first or not g_raw:
                        continue
                    if "Ù…Ø±Ø¯" in g_raw:
                        male.add(first)
                    elif "Ø²Ù†" in g_raw:
                        female.add(first)
        except Exception as e:
            print("âš ï¸ name lexicon load error:", e)
    return male, female

MALE_NAMES, FEMALE_NAMES = load_name_lexicon()

def gender_from_first_name(first_name: str) -> str:
    n = norm(first_name)
    if not n:
        return ""
    if n in MALE_NAMES: return "Ù…Ø±Ø¯"
    if n in FEMALE_NAMES: return "Ø²Ù†"
    n_l = n.lower()
    if n_l in {x.lower() for x in MALE_NAMES}: return "Ù…Ø±Ø¯"
    if n_l in {x.lower() for x in FEMALE_NAMES}: return "Ø²Ù†"
    return ""

# -----------------------------------------------------------------------------
# Enhanced Skill and Interest Mapping
# -----------------------------------------------------------------------------
BUILTIN_SKILL_SYNS = {
    "Python": {"python","Ù¾Ø§ÛŒØªÙˆÙ†"},
    "SQL": {"sql","Ø§Ø³ Ú©ÛŒÙˆ Ø§Ù„","Ø§Ø³â€ŒÚ©ÛŒÙˆØ§Ù„"},
    "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†": {"machine learning","ml","ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†","Ù…Ø§Ø´ÛŒÙ† Ù„Ø±Ù†ÛŒÙ†Ú¯"},
    "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚": {"deep learning","Ø¯ÛŒÙ¾ Ù„Ø±Ù†ÛŒÙ†Ú¯","ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚"},
    "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ": {"ai","Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"},
    "Excel": {"excel","Ø§Ú©Ø³Ù„"},
    "Power BI": {"power bi","powerbi","Ù¾Ø§ÙˆØ± Ø¨ÛŒâ€ŒØ¢ÛŒ","Ù¾Ø§ÙˆØ± Ø¨ÛŒ Ø§ÛŒ"},
    "PLC": {"plc","Ù¾ÛŒ Ø§Ù„ Ø³ÛŒ"},
    "JavaScript": {"javascript","Ø¬Ø§ÙˆØ§Ø§Ø³Ú©Ø±ÛŒÙ¾Øª","js"},
    "React": {"react","Ø±ÛŒ Ø§Ú©Øª","Ø±ÛŒâ€ŒØ§Ú©Øª"},
    "Node.js": {"node","nodejs","node.js"},
    "Vue.js": {"vue","vuejs","vue.js"},
    "Angular": {"angular"},
    "Docker": {"docker","Ø¯Ø§Ú©Ø±"},
    "Kubernetes": {"kubernetes","k8s"},
    "AWS": {"aws","amazon web services"},
    "Azure": {"azure","Ù…Ø§ÛŒÚ©Ø±ÙˆØ³Ø§ÙØª Ø¢Ø²ÙˆØ±"},
    "Git": {"git","Ú¯ÛŒØª"},
    "Linux": {"linux","Ù„ÛŒÙ†ÙˆÚ©Ø³"},
    "Java": {"java","Ø¬Ø§ÙˆØ§"},
    "C++": {"c++","Ø³ÛŒ Ù¾Ù„Ø§Ø³ Ù¾Ù„Ø§Ø³"},
    "C#": {"c#","Ø³ÛŒ Ø´Ø§Ø±Ù¾"},
    "PHP": {"php","Ù¾ÛŒ Ø§Ú† Ù¾ÛŒ"},
    "WordPress": {"wordpress","ÙˆØ±Ø¯Ù¾Ø±Ø³"},
    "Photoshop": {"photoshop","ÙØªÙˆØ´Ø§Ù¾"},
    "UI/UX Design": {"ui","ux","design","Ø·Ø±Ø§Ø­ÛŒ"},
    "Project Management": {"project management","Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ˜Ù‡"},
    "Data Analysis": {"data analysis","ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡"},
    "Business Intelligence": {"business intelligence","Ù‡ÙˆØ´ ØªØ¬Ø§Ø±ÛŒ"},
}

INTEREST_CATEGORIES = {
    "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†": {"Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†", "ai", "machine learning", "deep learning"},
    "Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±": {"Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ", "ØªÙˆØ³Ø¹Ù‡ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±", "programming", "software development", "coding"},
    "ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙˆÛŒ": {"ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡", "Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙˆÛŒ", "data analysis", "data mining", "big data"},
    "Ø·Ø±Ø§Ø­ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ ÙˆØ¨": {"Ø·Ø±Ø§Ø­ÛŒ ÙˆØ¨", "ØªÙˆØ³Ø¹Ù‡ ÙˆØ¨", "web design", "web development", "frontend", "backend"},
    "Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ˜Ù‡ Ùˆ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±": {"Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ˜Ù‡", "Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±", "project management", "business", "Ø§Ø³ØªØ§Ø±ØªØ§Ù¾"},
    "Ø§Ù…Ù†ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª": {"Ø§Ù…Ù†ÛŒØª", "Ø§Ù…Ù†ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª", "cybersecurity", "security", "Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ"},
    "Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø´ÛŒØ§": {"Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø´ÛŒØ§", "iot", "internet of things"},
    "Ø±Ø¨Ø§ØªÛŒÚ© Ùˆ Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†": {"Ø±Ø¨Ø§ØªÛŒÚ©", "Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†", "robotics", "automation"},
    "Ø¨Ù„Ø§Ú©Ú†ÛŒÙ† Ùˆ Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„": {"Ø¨Ù„Ø§Ú©Ú†ÛŒÙ†", "Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„", "blockchain", "cryptocurrency"},
    "Ø±Ø§ÛŒØ§Ù†Ø´ Ø§Ø¨Ø±ÛŒ": {"Ø±Ø§ÛŒØ§Ù†Ø´ Ø§Ø¨Ø±ÛŒ", "cloud computing", "cloud"},
    "ØªÙˆØ³Ø¹Ù‡ Ù…ÙˆØ¨Ø§ÛŒÙ„": {"ØªÙˆØ³Ø¹Ù‡ Ù…ÙˆØ¨Ø§ÛŒÙ„", "mobile development", "android", "ios"},
    "Ø¨Ø§Ø²ÛŒ Ø³Ø§Ø²ÛŒ": {"Ø¨Ø§Ø²ÛŒ Ø³Ø§Ø²ÛŒ", "game development", "gaming"},
}

def extract_detailed_skills_and_interests(text: str) -> Dict[str, List[str]]:
    """Use LLM to extract detailed skills and interests from text"""
    if not ollama:
        return {"skills": [], "interests": []}
    
    prompt = f"""
    Ø§Ø² Ù…ØªÙ† Ø²ÛŒØ±ØŒ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ùˆ Ø¹Ù„Ø§ÛŒÙ‚ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†:
    
    "{text}"
    
    Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ØŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ùˆ ØªÙˆØ§Ù†Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ø¨Ø§Ø´Ø¯.
    Ø¹Ù„Ø§ÛŒÙ‚ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±ÛŒØŒ ØµÙ†Ø§ÛŒØ¹ØŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡ Ø¨Ø§Ø´Ø¯.
    
    Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª JSON Ø²ÛŒØ± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†:
    {{
        "skills": ["Ù„ÛŒØ³Øª Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ"],
        "interests": ["Ù„ÛŒØ³Øª Ø¹Ù„Ø§ÛŒÙ‚ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ"]
    }}
    
    ÙÙ‚Ø· JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
    """
    
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": prompt}],
            options={"temperature": 0.1}
        )
        
        # Extract JSON from response
        json_match = re.search(r'\{.*\}', response['message']['content'], re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            return {
                "skills": data.get("skills", []),
                "interests": data.get("interests", [])
            }
    except Exception as e:
        print("Skills/Interests extraction error:", e)
    
    return {"skills": [], "interests": []}

def prettify_and_dedup_list(items):
    seen = set()
    out = []
    for it in (items or []):
        t = normalize_spaces(normalize_digits(str(it))).lower()
        if not t:
            continue
        pretty = None
        for label, syns in BUILTIN_SKILL_SYNS.items():
            if any(re.search(rf"(?<![Ø¢-ÛŒa-z0-9]){re.escape(s)}(?![Ø¢-ÛŒa-z0-9])", t) for s in syns):
                pretty = label
                break
        final = pretty or it.strip()
        key = final.lower()
        if key not in seen:
            seen.add(key)
            out.append(final)
    return out

def categorize_interests(interests: List[str]) -> List[str]:
    """Categorize interests into broader categories"""
    categorized = set()
    for interest in interests:
        interest_lower = interest.lower()
        for category, keywords in INTEREST_CATEGORIES.items():
            if any(keyword in interest_lower for keyword in keywords):
                categorized.add(category)
    
    return list(categorized)

def list_to_csv(items):
    return ", ".join([x for x in (items or []) if str(x).strip()])

# -----------------------------------------------------------------------------
# Multi-turn Conversation System
# -----------------------------------------------------------------------------
class ConversationManager:
    def __init__(self):
        self.sessions = {}
        self.required_fields = [
            "first_name", "last_name", "age", "gender", 
            "experience_years", "city", "skills", "military_status", "interests"
        ]
        self.field_questions = {
            "first_name": "Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ú¯ÙˆÛŒÛŒØ¯:",
            "last_name": "Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ú¯ÙˆÛŒÛŒØ¯:",
            "age": "Ø³Ù† Ø´Ù…Ø§ Ú†Ù†Ø¯ Ø³Ø§Ù„ Ø§Ø³ØªØŸ",
            "gender": "Ø¬Ù†Ø³ÛŒØª Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ (Ù…Ø±Ø¯/Ø²Ù†)",
            "experience_years": "Ú†Ù†Ø¯ Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø± Ø¯Ø§Ø±ÛŒØ¯ØŸ",
            "city": "Ø¯Ø± Ú©Ø¯Ø§Ù… Ø´Ù‡Ø± Ø³Ø§Ú©Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
            "skills": "Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ ÙÙ†ÛŒ Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹: PythonØŒ SQLØŒ Ø·Ø±Ø§Ø­ÛŒ ÙˆØ¨)",
            "military_status": "ÙˆØ¶Ø¹ÛŒØª Ø³Ø±Ø¨Ø§Ø²ÛŒ Ø´Ù…Ø§ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ (Ø¯Ø§Ø±Ø¯/Ù†Ø¯Ø§Ø±Ø¯/Ù…Ø¹Ø§Ù/Ø¯Ø± Ø­Ø§Ù„ Ø®Ø¯Ù…Øª)",
            "interests": "Ø¨Ù‡ Ú†Ù‡ Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…ÙˆØ¶ÙˆØ¹Ø§ØªÛŒ Ø¹Ù„Ø§Ù‚Ù‡ Ø¯Ø§Ø±ÛŒØ¯ØŸ (Ù…Ø«Ù„Ø§Ù‹: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ ØªÙˆØ³Ø¹Ù‡ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ØŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡)"
        }
    
    def start_session(self, session_id: str):
        self.sessions[session_id] = {
            'collected_data': {},
            'current_field_index': 0,
            'completed': False
        }
        return self.get_next_question(session_id)
    
    def get_next_question(self, session_id: str) -> str:
        session = self.sessions.get(session_id)
        if not session or session['completed']:
            return "Ù…Ù…Ù†ÙˆÙ†! Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù…Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯."
        
        for i, field in enumerate(self.required_fields):
            if field not in session['collected_data']:
                session['current_field_index'] = i
                return self.field_questions[field]
        
        session['completed'] = True
        return "Ù…Ù…Ù†ÙˆÙ†! Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù…Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯."
    
    def process_response(self, session_id: str, user_message: str) -> Dict:
        session = self.sessions.get(session_id)
        if not session or session['completed']:
            return {"question": "Ù…Ù…Ù†ÙˆÙ†! Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù…Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯.", "completed": True}
        
        current_field = self.required_fields[session['current_field_index']]
        
        # Extract field value using LLM
        extracted_data = self.extract_field_value(current_field, user_message)
        if extracted_data:
            session['collected_data'].update(extracted_data)
        
        next_question = self.get_next_question(session_id)
        
        return {
            "question": next_question,
            "update_fields": extracted_data,
            "completed": session['completed']
        }
    
    def extract_field_value(self, field: str, text: str) -> Dict:
        """Use LLM to extract specific field value from text"""
        if not ollama:
            return {}
        
        prompt = f"""
        Ø§Ø² Ù…ØªÙ† Ø²ÛŒØ± ÙÙ‚Ø· Ù…Ù‚Ø¯Ø§Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ "{self.field_questions[field]}" Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†:
        Ù…ØªÙ†: {text}
        
        ÙÙ‚Ø· Ù…Ù‚Ø¯Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ.
        """
        
        try:
            response = ollama.chat(
                model=OLLAMA_MODEL,
                messages=[{"role": "user", "content": prompt}]
            )
            value = response['message']['content'].strip()
            
            # Post-process based on field type
            if field in ['age', 'experience_years']:
                value = to_int_or_empty(value)
            elif field == 'gender':
                value = 'Ù…Ø±Ø¯' if 'Ù…Ø±Ø¯' in value else 'Ø²Ù†' if 'Ø²Ù†' in value else ''
            elif field == 'military_status':
                if 'Ø¯Ø§Ø±Ø¯' in value:
                    value = 'Ø¯Ø§Ø±Ø¯'
                elif 'Ù†Ø¯Ø§Ø±Ø¯' in value:
                    value = 'Ù†Ø¯Ø§Ø±Ø¯'
                elif 'Ù…Ø¹Ø§Ù' in value:
                    value = 'Ù…Ø¹Ø§Ù'
                elif 'Ø®Ø¯Ù…Øª' in value:
                    value = 'Ø¯Ø± Ø­Ø§Ù„ Ø®Ø¯Ù…Øª'
                else:
                    value = ''
            
            return {field: value}
        except Exception as e:
            print(f"Field extraction error for {field}:", e)
            return {}

conversation_manager = ConversationManager()

# -----------------------------------------------------------------------------
# Document Parser
# -----------------------------------------------------------------------------
def extract_text_from_pdf(file_path: str) -> str:
    """Extract text from PDF file"""
    if not PyPDF2:
        return ""
    
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
    except Exception as e:
        print("PDF extraction error:", e)
        return ""

def extract_text_from_docx(file_path: str) -> str:
    """Extract text from DOCX file"""
    if not Document:
        return ""
    
    try:
        doc = Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        print("DOCX extraction error:", e)
        return ""

def parse_resume_content(text: str) -> Dict:
    """Use LLM to extract structured data from resume text"""
    if not ollama:
        return {}
    
    prompt = f"""
    Ù…ØªÙ† Ø±Ø²ÙˆÙ…Ù‡ Ø²ÛŒØ± Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†:
    
    {text}
    
    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†:
    - first_name (Ù†Ø§Ù…)
    - last_name (Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ) 
    - age (Ø³Ù†)
    - gender (Ø¬Ù†Ø³ÛŒØª)
    - experience_years (Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø±)
    - city (Ø´Ù‡Ø±)
    - military_status (ÙˆØ¶Ø¹ÛŒØª Ø³Ø±Ø¨Ø§Ø²ÛŒ)
    - skills (Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§)
    - interests (Ø¹Ù„Ø§ÛŒÙ‚)
    
    ÙÙ‚Ø· JSON Ø®Ø§Ù„Øµ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
    """
    
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Extract JSON from response
        json_match = re.search(r'\{.*\}', response['message']['content'], re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            return postprocess_llm_profile(data)
    except Exception as e:
        print("Resume parsing error:", e)
    
    return {}

# -----------------------------------------------------------------------------
# Enhanced LLM Extraction with Better Capabilities Detection
# -----------------------------------------------------------------------------
LLM_SYSTEM = """
ØªÙˆ ÛŒÚ© Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒÚ¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ù‡Ø³ØªÛŒ. ÙÙ‚Ø· ÛŒÚ© JSON Ø®Ø§Ù„Øµ Ùˆ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø› Ù‡ÛŒÚ† Ù…ØªÙ† Ø§Ø¶Ø§ÙÛŒ Ù†Ù†ÙˆÛŒØ³.
ÙÛŒÙ„Ø¯Ù‡Ø§ Ø¯Ù‚ÛŒÙ‚Ø§ Ø§ÛŒÙ†â€ŒÙ‡Ø§ Ù‡Ø³ØªÙ†Ø¯:
{
  "first_name": string,
  "last_name": string,
  "age": number | "",
  "gender": "Ù…Ø±Ø¯" | "Ø²Ù†" | "",
  "experience_years": number | "",
  "city": string | "",
  "military_status": "Ø¯Ø§Ø±Ø¯" | "Ù†Ø¯Ø§Ø±Ø¯" | "Ù…Ø¹Ø§Ù" | "Ø¯Ø± Ø­Ø§Ù„ Ø®Ø¯Ù…Øª" | "",
  "skills": string[],        // ÙÙ‡Ø±Ø³Øª Ø¯Ù‚ÛŒÙ‚ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ØŒ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§
  "interests": string[]      // ÙÙ‡Ø±Ø³Øª Ø¯Ù‚ÛŒÙ‚ Ø¹Ù„Ø§ÛŒÙ‚ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±ÛŒ
}
Ù‚ÙˆØ§Ø¹Ø¯:
- Ø¨Ø±Ø§ÛŒ skills: ØªÙ…Ø§Ù… Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒØŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
- Ø¨Ø±Ø§ÛŒ interests: Ø¹Ù„Ø§ÛŒÙ‚ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒØŒ Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±ÛŒ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡ØŒ ØµÙ†Ø§ÛŒØ¹ Ùˆ Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
- Ø§Ú¯Ø± Ø¬Ù†Ø³ÛŒØª ØµØ±Ø§Ø­ØªØ§ Ø°Ú©Ø± Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ø§Ø² Ù†Ø§Ù… Ú©ÙˆÚ†Ú© Ø¨ØªÙˆØ§Ù† Ø­Ø¯Ø³ Ø²Ø¯ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù…Ù†Ø§Ø³Ø¨ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡.
- Ø§Ú¯Ø± Ú†ÛŒØ²ÛŒ Ù…Ø¹Ù„ÙˆÙ… Ù†Ø¨ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± Ø®Ø§Ù„ÛŒ "" ÛŒØ§ Ø¢Ø±Ø§ÛŒÙ‡ Ø®Ø§Ù„ÛŒ [] Ø¨Ø¯Ù‡.
- ÙÙ‚Ø· JSON Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†.
"""

def build_llm_user_prompt(transcript: str) -> str:
    txt = normalize_spaces(normalize_digits(transcript or ""))
    
    examples = [
        {
            "input": "Ù…Ù† Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ Û²Û¸ Ø³Ø§Ù„Ù…Ù‡ØŒ Û´ Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø± Ø¯Ø§Ø±Ù…ØŒ Ø³Ø§Ú©Ù† ØªÙ‡Ø±Ø§Ù†. Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§Ù… Ù¾Ø§ÛŒØªÙˆÙ† Ùˆ SQL. Ø¹Ù„Ø§ÛŒÙ‚: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ.",
            "output": {
                "first_name":"Ø¹Ù„ÛŒ","last_name":"Ø±Ø¶Ø§ÛŒÛŒ","age":28,"gender":"Ù…Ø±Ø¯",
                "experience_years":4,"city":"ØªÙ‡Ø±Ø§Ù†","military_status":"",
                "skills":["Python","SQL"],"interests":["Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"]
            }
        },
        {
            "input": "I am Sara Mohammadi, 25 years old with 3 years experience in web development. I know JavaScript, React, and Node.js. Interested in AI and data science.",
            "output": {
                "first_name":"Ø³Ø§Ø±Ø§","last_name":"Ù…Ø­Ù…Ø¯ÛŒ","age":25,"gender":"Ø²Ù†",
                "experience_years":3,"city":"","military_status":"",
                "skills":["JavaScript","React","Node.js"],"interests":["Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ","Ø¹Ù„Ù… Ø¯Ø§Ø¯Ù‡"]
            }
        }
    ]
    
    return (
        "Ø±ÙˆÙ†ÙˆØ´Øª Ú¯ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±:\n"
        + txt
        + "\n\nÙ†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ Ø¯Ø±Ø³Øª (Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ):\n"
        + json.dumps(examples, ensure_ascii=False)
        + "\n\nØ§Ú©Ù†ÙˆÙ† ÙÙ‚Ø· JSON Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†."
    )

def extract_json_block(text: str) -> dict:
    m = re.search(r"\{.*\}", text, flags=re.S)
    if not m:
        raise ValueError("No JSON block found")
    return json.loads(m.group(0))

def llm_extract(transcript: str) -> dict:
    if not ollama:
        raise RuntimeError("Ollama module not available.")
    
    # Language detection and translation
    translated_text = detect_and_translate(transcript)
    
    resp = ollama.chat(
        model=OLLAMA_MODEL,
        messages=[
            {"role": "system", "content": LLM_SYSTEM.strip()},
            {"role": "user", "content": build_llm_user_prompt(translated_text)}
        ],
        options={"temperature": 0.1}
    )
    raw = (resp["message"]["content"] or "").strip()
    return extract_json_block(raw)

def postprocess_llm_profile(obj: dict) -> dict:
    obj = obj or {}
    profile = {
        "first_name": norm(obj.get("first_name")),
        "last_name": norm(obj.get("last_name")),
        "age": to_int_or_empty(obj.get("age")),
        "gender": norm(obj.get("gender")),
        "experience_years": to_int_or_empty(obj.get("experience_years")),
        "city": norm(obj.get("city")),
        "military_status": norm(obj.get("military_status")),
        "skills": list(obj.get("skills") or []),
        "interests": list(obj.get("interests") or []),
    }

    # Smart error correction for gender
    if profile["gender"] == "":
        g = gender_from_first_name(profile["first_name"])
        if g:
            profile["gender"] = g

    # Enhanced skills and interests extraction
    if not profile["skills"] or not profile["interests"]:
        # Combine all text for better extraction
        combined_text = f"{profile['first_name']} {profile['last_name']} {profile['experience_years']} Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡"
        detailed_extraction = extract_detailed_skills_and_interests(combined_text)
        
        if not profile["skills"] and detailed_extraction["skills"]:
            profile["skills"] = detailed_extraction["skills"]
        
        if not profile["interests"] and detailed_extraction["interests"]:
            profile["interests"] = detailed_extraction["interests"]

    # Pretty & dedup lists
    profile["skills"] = prettify_and_dedup_list(profile["skills"])
    profile["interests"] = prettify_and_dedup_list(profile["interests"])
    
    # Categorize interests for better recommendations
    if profile["interests"]:
        categorized = categorize_interests(profile["interests"])
        if categorized:
            profile["interests"] = categorized

    # Convert lists to CSV for the form inputs in UI
    profile["skills"] = list_to_csv(profile["skills"])
    profile["interests"] = list_to_csv(profile["interests"])
    return profile

# -----------------------------------------------------------------------------
# Enhanced AI Job Recommendations and Summary Generation
# -----------------------------------------------------------------------------
def generate_job_recommendations(profile: Dict) -> str:
    """Generate detailed job recommendations based on skills and experience"""
    if not ollama:
        return "Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
    
    skills = profile.get('skills', '')
    experience = profile.get('experience_years', 0)
    interests = profile.get('interests', '')
    
    prompt = f"""
    Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø´Ø®ØµØ§Øª Ø²ÛŒØ±ØŒ Û³-Û´ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†:
    
    Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§: {skills}
    Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø±: {experience} Ø³Ø§Ù„
    Ø¹Ù„Ø§ÛŒÙ‚: {interests}
    
    Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯:
    - Ø¹Ù†ÙˆØ§Ù† Ø´ØºÙ„ÛŒ Ø¯Ù‚ÛŒÙ‚
    - ØµÙ†Ø¹Øª Ù…Ø±Ø¨ÙˆØ·Ù‡
    - Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    - Ù…Ø³ÛŒØ± Ø±Ø´Ø¯ Ø´ØºÙ„ÛŒ
    
    Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÙÙ‡Ø±Ø³Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ùˆ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†.
    """
    
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": prompt}],
            options={"temperature": 0.7}
        )
        return response['message']['content']
    except Exception as e:
        print("Job recommendations error:", e)
        return "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

def generate_applicant_summary(profile: Dict) -> str:
    """Generate a professional summary of the applicant"""
    if not ollama:
        return "Ø³Ø±ÙˆÛŒØ³ ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
    
    prompt = f"""
    ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ÛŒÚ© Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙØ±Ø¯ Ø¨Ù†ÙˆÛŒØ³ Ú©Ù‡ Ø´Ø§Ù…Ù„:
    - Ù…Ø¹Ø±ÙÛŒ Ú©Ù„ÛŒ
    - ØªØ®ØµØµâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    - Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯ÛŒ
    - Ù¾ØªØ§Ù†Ø³ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±Ø´Ø¯
    
    Ù…Ø´Ø®ØµØ§Øª:
    Ù†Ø§Ù…: {profile.get('first_name', '')} {profile.get('last_name', '')}
    Ø³Ù†: {profile.get('age', '')}
    Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø±: {profile.get('experience_years', '')} Ø³Ø§Ù„
    Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§: {profile.get('skills', '')}
    Ø¹Ù„Ø§ÛŒÙ‚: {profile.get('interests', '')}
    
    Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒØŒ Ø¬Ø°Ø§Ø¨ Ùˆ Ù…Ø®ØªØµØ± Ø¨Ø§Ø´Ø¯.
    """
    
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": prompt}],
            options={"temperature": 0.3}
        )
        return response['message']['content']
    except Exception as e:
        print("Summary generation error:", e)
        return "Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

# -----------------------------------------------------------------------------
# Excel I/O
# -----------------------------------------------------------------------------
COLUMNS = [
    "Ù†Ø§Ù…", "Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ", "Ø³Ù†", "Ø¬Ù†Ø³ÛŒØª",
    "ØªØ¹Ø¯Ø§Ø¯ Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø±", "Ø´Ù‡Ø± Ù…Ø­Ù„ Ø³Ú©ÙˆÙ†Øª", "ÙˆØ¶Ø¹ÛŒØª Ø³Ø±Ø¨Ø§Ø²ÛŒ",
    "Ù…Ù‡Ø§Ø±Øª Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ", "Ø¹Ù„Ø§ÛŒÙ‚", "Ø«Ø¨Øª Ø¯Ø±"
]

def append_record_to_excel(row: dict, xlsx_path: str):
    df_row = pd.DataFrame([{
        "Ù†Ø§Ù…": row.get("first_name", ""),
        "Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ": row.get("last_name", ""),
        "Ø³Ù†": row.get("age", ""),
        "Ø¬Ù†Ø³ÛŒØª": row.get("gender", ""),
        "ØªØ¹Ø¯Ø§Ø¯ Ø³Ø§Ù„ Ø³Ø§Ø¨Ù‚Ù‡ Ú©Ø§Ø±": row.get("experience_years", ""),
        "Ø´Ù‡Ø± Ù…Ø­Ù„ Ø³Ú©ÙˆÙ†Øª": row.get("city", ""),
        "ÙˆØ¶Ø¹ÛŒØª Ø³Ø±Ø¨Ø§Ø²ÛŒ": row.get("military_status", ""),
        "Ù…Ù‡Ø§Ø±Øª Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ": row.get("skills", ""),
        "Ø¹Ù„Ø§ÛŒÙ‚": row.get("interests", ""),
        "Ø«Ø¨Øª Ø¯Ø±": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }], columns=COLUMNS)

    if os.path.exists(xlsx_path):
        try:
            old = pd.read_excel(xlsx_path)
            merged = pd.concat([old, df_row], ignore_index=True)
        except Exception:
            merged = df_row
    else:
        merged = df_row

    with pd.ExcelWriter(xlsx_path, engine="openpyxl") as writer:
        merged.to_excel(writer, index=False)

# -----------------------------------------------------------------------------
# Routes
# -----------------------------------------------------------------------------
@app.route("/", methods=["GET", "POST"])
def index():
    last_record = None
    success = False

    if request.method == "POST":
        payload = {
            "first_name": norm(request.form.get("first_name")),
            "last_name": norm(request.form.get("last_name")),
            "age": to_int_or_empty(request.form.get("age")),
            "gender": norm(request.form.get("gender")),
            "experience_years": to_int_or_empty(request.form.get("experience_years")),
            "city": norm(request.form.get("city")),
            "military_status": norm(request.form.get("military_status")),
            "skills": norm(request.form.get("skills")),
            "interests": norm(request.form.get("interests")),
        }
        append_record_to_excel(payload, app.config["EXCEL_PATH"])
        last_record = payload
        success = True

    return render_template(
        "index.html",
        success=success,
        last_record=last_record,
        excel_rel_path=os.path.relpath(app.config["EXCEL_PATH"]).replace("\\", "/"),
    )

@app.route("/nlp/parse", methods=["POST"])
def nlp_parse():
    """Enhanced NLP parsing with better capabilities extraction"""
    if not ollama:
        return jsonify({"error": "ollama_not_available"}), 500

    data = request.get_json(silent=True) or {}
    utter = normalize_spaces(normalize_digits(norm(data.get("utterance"))))
    if not utter:
        return jsonify({"error": "empty utterance"}), 400

    try:
        raw_profile = llm_extract(utter)
        profile = postprocess_llm_profile(raw_profile)
        return jsonify(profile), 200
    except Exception as e:
        print("âš ï¸ LLM extraction error:", e)
        empty = {
            "first_name":"", "last_name":"", "age":"", "gender":"", "experience_years":"",
            "city":"", "military_status":"", "skills":"", "interests":""
        }
        return jsonify(empty), 200

@app.route("/conversation/start", methods=["POST"])
def start_conversation():
    """Start a new conversation session"""
    session_id = request.remote_addr  # Simple session ID
    question = conversation_manager.start_session(session_id)
    return jsonify({"question": question})

@app.route("/conversation/respond", methods=["POST"])
def conversation_respond():
    """Process user response in conversation"""
    session_id = request.remote_addr
    data = request.get_json(silent=True) or {}
    user_message = data.get("message", "")
    
    result = conversation_manager.process_response(session_id, user_message)
    return jsonify(result)

@app.route("/parse/resume", methods=["POST"])
def parse_resume():
    """Parse uploaded resume file"""
    if 'resume' not in request.files:
        return jsonify({"success": False, "error": "No file uploaded"}), 400
    
    file = request.files['resume']
    if file.filename == '':
        return jsonify({"success": False, "error": "No file selected"}), 400
    
    # Save uploaded file
    filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
    file_path = os.path.join(app.config["UPLOAD_FOLDER"], filename)
    file.save(file_path)
    
    try:
        # Extract text based on file type
        if file.filename.lower().endswith('.pdf'):
            text = extract_text_from_pdf(file_path)
        elif file.filename.lower().endswith(('.doc', '.docx')):
            text = extract_text_from_docx(file_path)
        else:
            return jsonify({"success": False, "error": "Unsupported file format"}), 400
        
        if not text.strip():
            return jsonify({"success": False, "error": "Could not extract text from file"}), 400
        
        # Parse resume content
        fields = parse_resume_content(text)
        
        # Clean up uploaded file
        os.remove(file_path)
        
        return jsonify({"success": True, "fields": fields})
        
    except Exception as e:
        print("Resume parsing error:", e)
        if os.path.exists(file_path):
            os.remove(file_path)
        return jsonify({"success": False, "error": str(e)}), 500

@app.route("/ai/recommend-jobs", methods=["POST"])
def recommend_jobs():
    """Generate job recommendations based on profile"""
    data = request.get_json(silent=True) or {}
    recommendations = generate_job_recommendations(data)
    return jsonify({"recommendations": recommendations})

@app.route("/ai/generate-summary", methods=["POST"])
def generate_summary():
    """Generate applicant summary"""
    data = request.get_json(silent=True) or {}
    summary = generate_applicant_summary(data)
    return jsonify({"summary": summary})

# -----------------------------------------------------------------------------
# Entrypoint
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    print("ğŸš€ I-SELECT Enhanced Server Starting...")
    print("ğŸ“ Features: Multi-turn Conversation, Real-time STT, Document Parsing, AI Recommendations")
    print("ğŸ”Š Make sure Ollama is running with Gemma model")
    app.run(debug=True, port=5001)